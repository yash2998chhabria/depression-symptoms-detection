{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "bidirectional_Multilayer_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoRp3XtIx4j8",
        "colab_type": "code",
        "outputId": "dae75096-652c-4136-fa0e-a0f18ef5d169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (47.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOA24701iTU",
        "colab_type": "code",
        "outputId": "81f689dd-fd58-4b93-c756-a9014c9f2eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmoRf88Kx4kD",
        "colab_type": "code",
        "outputId": "b985a5d3-e73f-4b1b-b974-bc1124bad145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLetcTBIx4kH",
        "colab_type": "code",
        "outputId": "da2903d8-4caa-4bc3-e90a-63ee31360b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy \n",
        "\n",
        "spacy.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dG38OUDx4kL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "534add2c-874e-4dfa-ba7c-4d9dd8ab8b69"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mzb7hXKx4kQ",
        "colab_type": "code",
        "outputId": "6315e207-1fd8-4f5b-f1a8-360c9f422eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "tweets = pd.read_csv('/content/drive/My Drive/depressionrnn/all_nobias.csv', error_bad_lines = False)\n",
        "\n",
        "tweets.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My 7th year in that school (13years old) was g...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“Clash of the titans!” Caw exclaimed happily.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I hate it.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Starving because I just couldn't leave the saf...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>](http://www.esquire.com/news-politics/a23772/...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  condition\n",
              "0  My 7th year in that school (13years old) was g...        0.0\n",
              "1      “Clash of the titans!” Caw exclaimed happily.        0.0\n",
              "2                                         I hate it.        1.0\n",
              "3  Starving because I just couldn't leave the saf...        1.0\n",
              "4  ](http://www.esquire.com/news-politics/a23772/...        0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCb31pkd2ECf",
        "colab_type": "code",
        "outputId": "3865dc1d-e4aa-43b2-8b90-28ba603e8d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "tweets = tweets.rename(index = str, columns = {'sentence': 'SentimentText', 'condition': 'Sentiment'})\n",
        "\n",
        "tweets.head()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My 7th year in that school (13years old) was g...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“Clash of the titans!” Caw exclaimed happily.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I hate it.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Starving because I just couldn't leave the saf...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>](http://www.esquire.com/news-politics/a23772/...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment\n",
              "0  My 7th year in that school (13years old) was g...        0.0\n",
              "1      “Clash of the titans!” Caw exclaimed happily.        0.0\n",
              "2                                         I hate it.        1.0\n",
              "3  Starving because I just couldn't leave the saf...        1.0\n",
              "4  ](http://www.esquire.com/news-politics/a23772/...        0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUwa5k7lhVCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets=tweets.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5EOmg0Ox4kV",
        "colab_type": "text"
      },
      "source": [
        "The dataframe consists of 4 columns and we want to use only ‘Sentiment’ and ‘SentimentText’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jFTyrxYx4ka",
        "colab_type": "code",
        "outputId": "620010bd-ae79-44f5-9a10-d2d97e032e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweets.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5731, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJU5RB4-x4ke",
        "colab_type": "code",
        "outputId": "61f70934-4c4a-4738-c2ee-88b00868a016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweets['Sentiment'].unique()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJZwLFefx4ki",
        "colab_type": "code",
        "outputId": "fa7e00f2-e0a2-4379-fc7f-3389345324d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "tweets.Sentiment.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    3904\n",
              "1.0    1827\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaPiBGVWx4kl",
        "colab_type": "code",
        "outputId": "86478dfb-fcbc-4789-9a60-68f1d45b3ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "ax = sns.barplot(x=tweets.Sentiment.unique(), y=tweets.Sentiment.value_counts())\n",
        "\n",
        "ax.set(xlabel='Labels')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 0, 'Labels')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAHgCAYAAACb58plAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAd00lEQVR4nO3df7Dl9V3f8dfbXSCxiYGEla67xMVkO5ZUQ+JK0LQWYeRXbTd2YkraypoyXTslrVYbBetITMLU39TYiF1lBRwNwUSb1UEREzRNxxCWhBAIYm7zo+xKwiYQNEWJkHf/uN81J5u9y124n3v3ro/HzJl7zuf7/Z77vv/sPOe73/M91d0BAACW1pet9AAAAHA0EtoAADCA0AYAgAGENgAADCC0AQBgAKENAAADrF3pAUY48cQTe9OmTSs9BgAAR7nbb7/9U9297mDbjsrQ3rRpU3bv3r3SYwAAcJSrqo8vtM2lIwAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMMD+2qWlNV76+q35len1JVt1bVXFW9paqOndaPm17PTds3zbzHZdP6vVV17uiZAQDgqVqOM9rfm+Semdc/keTK7n5+koeSXDytX5zkoWn9ymm/VNWpSS5M8oIk5yX5hapaswxzAwDAkzY0tKtqY5J/kuSXp9eV5Kwkb512uTbJy6bnW6fXmbafPe2/Ncn13f1od380yVyS00fODQAAT9XoM9r/LckPJvn89Po5ST7T3Y9Nr/ck2TA935DkviSZtj887f836wc55m9U1faq2l1Vu/ft27fUfwcAAByWYaFdVd+e5IHuvn3U75jV3Tu6e0t3b1m37qDfggkAAMtm5FewvzTJP6uqC5I8LclXJPm5JMdX1drprPXGJHun/fcmOTnJnqpam+RZST49s77f7DEAAHBEGnZGu7sv6+6N3b0p8x9mfGd3/6sktyR5+bTbtiRvn57vml5n2v7O7u5p/cLpriSnJNmc5L2j5gYAgKUw8oz2Qn4oyfVV9YYk709y9bR+dZJfraq5JA9mPs7T3XdX1Q1JPpTksSSXdPfjyz82AAAsXs2fND66bNmypXfv3r3SYwAAcJSrqtu7e8vBtvlmSAAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAFW4vZ+f2t8w2uuW+kRgFXi9p+6aKVHAGCJOaMNAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYaFdlU9rareW1UfqKq7q+rHpvVrquqjVXXH9DhtWq+qemNVzVXVnVX14pn32lZVH54e20bNDAAAS2XtwPd+NMlZ3f3Zqjomybur6nenba/p7rcesP/5STZPj5ckuSrJS6rq2UkuT7IlSSe5vap2dfdDA2cHAICnZNgZ7Z732enlMdOjD3HI1iTXTce9J8nxVbU+yblJbu7uB6e4vjnJeaPmBgCApTD0Gu2qWlNVdyR5IPOxfOu06Yrp8pArq+q4aW1DkvtmDt8zrS20fuDv2l5Vu6tq9759+5b8bwEAgMMxNLS7+/HuPi3JxiSnV9U/SHJZkq9N8o1Jnp3kh5bod+3o7i3dvWXdunVL8ZYAAPCkLctdR7r7M0luSXJed98/XR7yaJJfSXL6tNveJCfPHLZxWltoHQAAjlgj7zqyrqqOn54/Pcm3JfmT6brrVFUleVmSu6ZDdiW5aLr7yBlJHu7u+5PclOScqjqhqk5Ics60BgAAR6yRdx1Zn+TaqlqT+aC/obt/p6reWVXrklSSO5L8u2n/G5NckGQuySNJXpUk3f1gVb0+yW3Tfq/r7gcHzg0AAE/ZsNDu7juTvOgg62ctsH8nuWSBbTuT7FzSAQEAYCDfDAkAAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYYFtpV9bSqem9VfaCq7q6qH5vWT6mqW6tqrqreUlXHTuvHTa/npu2bZt7rsmn93qo6d9TMAACwVEae0X40yVnd/cIkpyU5r6rOSPITSa7s7ucneSjJxdP+Fyd5aFq/ctovVXVqkguTvCDJeUl+oarWDJwbAACesmGh3fM+O708Znp0krOSvHVavzbJy6bnW6fXmbafXVU1rV/f3Y9290eTzCU5fdTcAACwFIZeo11Va6rqjiQPJLk5yf9J8pnufmzaZU+SDdPzDUnuS5Jp+8NJnjO7fpBjAADgiDQ0tLv78e4+LcnGzJ+F/tpRv6uqtlfV7qravW/fvlG/BgAAFmVZ7jrS3Z9JckuSb0pyfFWtnTZtTLJ3er43yclJMm1/VpJPz64f5JjZ37Gju7d095Z169YN+TsAAGCxRt51ZF1VHT89f3qSb0tyT+aD++XTbtuSvH16vmt6nWn7O7u7p/ULp7uSnJJkc5L3jpobAACWwton3uVJW5/k2ukOIV+W5Ibu/p2q+lCS66vqDUnen+Tqaf+rk/xqVc0leTDzdxpJd99dVTck+VCSx5Jc0t2PD5wbAACesmGh3d13JnnRQdY/koPcNaS7/yrJdy7wXlckuWKpZwQAgFF8MyQAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhgWGhX1clVdUtVfaiq7q6q753WX1tVe6vqjulxwcwxl1XVXFXdW1XnzqyfN63NVdWlo2YGAIClsnbgez+W5Ae6+31V9cwkt1fVzdO2K7v7p2d3rqpTk1yY5AVJvirJH1TV35s2vynJtyXZk+S2qtrV3R8aODsAADwlw0K7u+9Pcv/0/C+q6p4kGw5xyNYk13f3o0k+WlVzSU6fts1190eSpKqun/YV2gAAHLGW5RrtqtqU5EVJbp2WXl1Vd1bVzqo6YVrbkOS+mcP2TGsLrQMAwBFreGhX1TOSvC3J93X3nye5KsnzkpyW+TPeP7NEv2d7Ve2uqt379u1bircEAIAnbWhoV9UxmY/sX+vu30yS7v5kdz/e3Z9P8kv5wuUhe5OcPHP4xmltofUv0t07untLd29Zt27d0v8xAABwGEbedaSSXJ3knu7+2Zn19TO7fUeSu6bnu5JcWFXHVdUpSTYneW+S25JsrqpTqurYzH9gcteouQEAYCmMvOvIS5N8V5IPVtUd09oPJ3llVZ2WpJN8LMn3JEl3311VN2T+Q46PJbmkux9Pkqp6dZKbkqxJsrO77x44NwAAPGUj7zry7iR1kE03HuKYK5JccZD1Gw91HAAAHGl8MyQAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBggEWFdlW9dDFrAADAvMWe0f75Ra4BAABJ1h5qY1V9U5JvTrKuqr5/ZtNXJFkzcjAAAFjNDhnaSY5N8oxpv2fOrP95kpePGgoAAFa7Q4Z2d/9Rkj+qqmu6++PLNBMAAKx6T3RGe7/jqmpHkk2zx3T3WSOGAgCA1W6xof0bSX4xyS8neXzcOAAAcHRYbGg/1t1XDZ0EAACOIou9vd9vV9W/r6r1VfXs/Y+hkwEAwCq22DPa26afr5lZ6yRfs7TjAADA0WFRod3dp4weBAAAjiaL/Qr2L6+qH5nuPJKq2lxV3z52NAAAWL0We432ryT5XOa/JTJJ9iZ5w5CJAADgKLDY0H5ed/9kkr9Oku5+JEkNmwoAAFa5xYb256rq6Zn/AGSq6nlJHh02FQAArHKLvevI5Ul+L8nJVfVrSV6a5LtHDQUAAKvdYu86cnNVvS/JGZm/ZOR7u/tTQycDAIBVbLGXjiTJhiRrkhyb5Fuq6p+PGQkAAFa/RZ3RrqqdSb4+yd1JPj8td5LfHDQXAACsaou9RvuM7j516CQAAHAUWeylI39cVUIbAAAWabFntK/LfGx/IvO39ask3d1fP2wyAABYxRYb2lcn+a4kH8wXrtEGAAAWsNjQ3tfdu4ZOAgAAR5HFhvb7q+rXk/x2Zr4RsrvddQQAAA5isaH99MwH9jkza27vBwAAC1jsN0O+6nDfuKpOzvyHKE/KfJTv6O6fq6pnJ3lLkk1JPpbkFd39UFVVkp9LckGSR5J8d3e/b3qvbUl+ZHrrN3T3tYc7DwAALKdDhnZV/WB3/2RV/XzmY/mLdPd/PMThjyX5ge5+X1U9M8ntVXVzku9O8o7u/vGqujTJpUl+KMn5STZPj5ckuSrJS6YwvzzJlmmG26tqV3c/dJh/KwAALJsnOqN9z/Rz9+G+cXffn+T+6flfVNU9mf8a961Jzpx2uzbJH2Y+tLcmua67O8l7qur4qlo/7Xtzdz+YJFOsn5fkzYc7EwAALJdDhnZ3//b09JHu/o3ZbVX1nYv9JVW1KcmLktya5KQpwpPkE5m/tCSZj/D7Zg7bM60ttA4AAEesxX4z5GWLXPsSVfWMJG9L8n3d/eez26az119yScqTUVXbq2p3Ve3et2/fUrwlAAA8aU90jfb5mf9w4oaqeuPMpq/I/DXYh1RVx2Q+sn9t5laAn6yq9d19/3RpyAPT+t4kJ88cvnFa25svXGqyf/0PD/xd3b0jyY4k2bJly5LEOwAAPFlPdEb7zzJ/ffZfJbl95rErybmHOnC6i8jVSe7p7p+d2bQrybbp+bYkb59Zv6jmnZHk4ekSk5uSnFNVJ1TVCZm/xeBNi/z7AABgRTzRNdofSPKBqvr17v7rw3zvl2b62vaqumNa++EkP57khqq6OMnHk7xi2nZj5s+ez2X+9n6vmmZ4sKpen+S2ab/X7f9gJAAAHKkW+4U1p1fVa5N89XRMZf4S669Z6IDufve038GcfZD9O8klC7zXziQ7FzkrAACsuMWG9tVJ/lPmLxt5fNw4AABwdFhsaD/c3b87dBIAADiKLDa0b6mqn0rym0ke3b+4/yvSAQCAL7bY0H7J9HPLzFonOWtpxwEAgKPDokK7u7919CAAAHA0WdQ3Q1bVSVV1dVX97vT61On2fAAAwEEs9ivYr8n8l8R81fT6T5N834iBAADgaLDY0D6xu29I8vkk6e7H4jZ/AACwoMWG9v+rqudk/gOQ2f8V6cOmAgCAVW6xdx35/iS7kjyvqv53knVJXj5sKgAAWOUOeUa7qr6xqv7udL/sf5zkhzN/H+3fT7JnGeYDAIBV6YkuHfkfST43Pf/mJP8lyZuSPJRkx8C5AABgVXuiS0fWdPeD0/N/kWRHd78tyduq6o6xowEAwOr1hKFdVWunu4ycnWT7YRwLAIft/77u61Z6BGCVeO6PfnClRzikJ4rlNyf5o6r6VJK/TPK/kqSqnh93HQEAgAUdMrS7+4qqekeS9Ul+v7t72vRlSf7D6OEAAGC1esLLP7r7PQdZ+9Mx4wAAwNFhsV9YAwAAHAahDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYYFhoV9XOqnqgqu6aWXttVe2tqjumxwUz2y6rqrmqureqzp1ZP29am6uqS0fNCwAAS2nkGe1rkpx3kPUru/u06XFjklTVqUkuTPKC6ZhfqKo1VbUmyZuSnJ/k1CSvnPYFAIAj2tpRb9zd76qqTYvcfWuS67v70SQfraq5JKdP2+a6+yNJUlXXT/t+aInHBQCAJbUS12i/uqrunC4tOWFa25Dkvpl99kxrC60DAMARbblD+6okz0tyWpL7k/zMUr1xVW2vqt1VtXvfvn1L9bYAAPCkLGtod/cnu/vx7v58kl/KFy4P2Zvk5JldN05rC60f7L13dPeW7t6ybt26pR8eAAAOw7KGdlWtn3n5HUn235FkV5ILq+q4qjolyeYk701yW5LNVXVKVR2b+Q9M7lrOmQEA4MkY9mHIqnpzkjOTnFhVe5JcnuTMqjotSSf5WJLvSZLuvruqbsj8hxwfS3JJdz8+vc+rk9yUZE2Snd1996iZAQBgqYy868grD7J89SH2vyLJFQdZvzHJjUs4GgAADOebIQEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwADDQruqdlbVA1V118zas6vq5qr68PTzhGm9quqNVTVXVXdW1Ytnjtk27f/hqto2al4AAFhKI89oX5PkvAPWLk3yju7enOQd0+skOT/J5umxPclVyXyYJ7k8yUuSnJ7k8v1xDgAAR7Jhod3d70ry4AHLW5NcOz2/NsnLZtav63nvSXJ8Va1Pcm6Sm7v7we5+KMnN+dJ4BwCAI85yX6N9UnffPz3/RJKTpucbktw3s9+eaW2hdQAAOKKt2Ichu7uT9FK9X1Vtr6rdVbV73759S/W2AADwpCx3aH9yuiQk088HpvW9SU6e2W/jtLbQ+pfo7h3dvaW7t6xbt27JBwcAgMOx3KG9K8n+O4dsS/L2mfWLpruPnJHk4ekSk5uSnFNVJ0wfgjxnWgMAgCPa2lFvXFVvTnJmkhOrak/m7x7y40luqKqLk3w8ySum3W9MckGSuSSPJHlVknT3g1X1+iS3Tfu9rrsP/IAlAAAccYaFdne/coFNZx9k305yyQLvszPJziUcDQAAhvPNkAAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYIAVCe2q+lhVfbCq7qiq3dPas6vq5qr68PTzhGm9quqNVTVXVXdW1YtXYmYAADgcK3lG+1u7+7Tu3jK9vjTJO7p7c5J3TK+T5Pwkm6fH9iRXLfukAABwmI6kS0e2Jrl2en5tkpfNrF/X896T5PiqWr8SAwIAwGKtVGh3kt+vqturavu0dlJ33z89/0SSk6bnG5LcN3PsnmkNAACOWGtX6Pf+w+7eW1VfmeTmqvqT2Y3d3VXVh/OGU7BvT5LnPve5SzcpAAA8CStyRru7904/H0jyW0lOT/LJ/ZeETD8fmHbfm+TkmcM3TmsHvueO7t7S3VvWrVs3cnwAAHhCyx7aVfV3quqZ+58nOSfJXUl2Jdk27bYtydun57uSXDTdfeSMJA/PXGICAABHpJW4dOSkJL9VVft//6939+9V1W1Jbqiqi5N8PMkrpv1vTHJBkrkkjyR51fKPDAAAh2fZQ7u7P5LkhQdZ/3SSsw+y3kkuWYbRAABgyRxJt/cDAICjhtAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBgAKENAAADCG0AABhAaAMAwABCGwAABhDaAAAwgNAGAIABhDYAAAwgtAEAYAChDQAAAwhtAAAYQGgDAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADCC0AQBggFUT2lV1XlXdW1VzVXXpSs8DAACHsipCu6rWJHlTkvOTnJrklVV16spOBQAAC1sVoZ3k9CRz3f2R7v5ckuuTbF3hmQAAYEGrJbQ3JLlv5vWeaQ0AAI5Ia1d6gKVSVduTbJ9efraq7l3JeeAQTkzyqZUegiNL/fS2lR4BjnT+7eRLXV4rPUGSfPVCG1ZLaO9NcvLM643T2t/o7h1JdiznUPBkVNXu7t6y0nMArCb+7WQ1Wi2XjtyWZHNVnVJVxya5MMmuFZ4JAAAWtCrOaHf3Y1X16iQ3JVmTZGd3373CYwEAwIJWRWgnSXffmOTGlZ4DloBLnAAOn387WXWqu1d6BgAAOOqslmu0AQBgVRHaMEhVnVdV91bVXFVdepDtx1XVW6btt1bVpuWfEuDIUVU7q+qBqrprge1VVW+c/t28s6pevNwzwuEQ2jBAVa1J8qYk5yc5Nckrq+rUA3a7OMlD3f38JFcm+YnlnRLgiHNNkvMOsf38JJunx/YkVy3DTPCkCW0Y4/Qkc939ke7+XJLrk2w9YJ+tSa6dnr81ydlVdUTceR9gJXT3u5I8eIhdtia5rue9J8nxVbV+eaaDwye0YYwNSe6beb1nWjvoPt39WJKHkzxnWaYDWJ0W828rHDGENgAADCC0YYy9SU6eeb1xWjvoPlW1Nsmzknx6WaYDWJ0W828rHDGENoxxW5LNVXVKVR2b5MIkuw7YZ1eSbdPzlyd5Z7uxPcCh7Epy0XT3kTOSPNzd96/0ULCQVfPNkLCadPdjVfXqJDclWZNkZ3ffXVWvS7K7u3cluTrJr1bVXOY//HPhyk0MsPKq6s1JzkxyYlXtSXJ5kmOSpLt/MfPfEH1BkrkkjyR51cpMCovjmyEBAGAAl44AAMAAQhsAAAYQ2gAAMIDQBgCAAYQ2AAAMILQBjiJV9dnD2Pe1VfWfR70/wN92QhsAAAYQ2gBHuar6p1V1a1W9v6r+oKpOmtn8wqr646r6cFX925ljXlNVt1XVnVX1Ywd5z/VV9a6quqOq7qqqf7QsfwzAKiK0AY5+705yRne/KMn1SX5wZtvXJzkryTcl+dGq+qqqOifJ5iSnJzktyTdU1bcc8J7/MslN3X1akhcmuWPw3wCw6vgKdoCj38Ykb6mq9UmOTfLRmW1v7+6/TPKXVXVL5uP6HyY5J8n7p32ekfnwftfMcbcl2VlVxyT5n90ttAEO4Iw2wNHv55P89+7+uiTfk+RpM9v6gH07SSX5r9192vR4fndf/UU7db8rybck2Zvkmqq6aNz4AKuT0AY4+j0r80GcJNsO2La1qp5WVc9Jcmbmz1TflOTfVNUzkqSqNlTVV84eVFVfneST3f1LSX45yYsHzg+wKrl0BODo8uVVtWfm9c8meW2S36iqh5K8M8kpM9vvTHJLkhOTvL67/yzJn1XV30/yx1WVJJ9N8q+TPDBz3JlJXlNVfz1td0Yb4ADVfeD/GgIAAE+VS0cAAGAAoQ0AAAMIbQAAGEBoAwDAAEIbAAAGENoAADCA0AYAgAGENgAADPD/AUQqYRSwNihwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRv0G_Vcx4kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(tweets, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq4YwB1gx4kt",
        "colab_type": "code",
        "outputId": "f50f0a3d-5a0e-4a46-f1a7-8631c7dfb375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "train.reset_index(drop=True), test.reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                          SentimentText  Sentiment\n",
              " 0     I keep trying and scrambling to pull myself ou...        0.0\n",
              " 1     She messages me in the morning and says I'm co...        1.0\n",
              " 2     “Let’s see, twentieth century Sol...” she muse...        0.0\n",
              " 3     He could feel the eyes of the marines under hi...        0.0\n",
              " 4     “The whole point of this excursion was to prov...        0.0\n",
              " ...                                                 ...        ...\n",
              " 4579                                        They cheat.        0.0\n",
              " 4580  My boyfriend is the most amazing person I've e...        0.0\n",
              " 4581  But its the only thing from the many things i ...        1.0\n",
              " 4582  But when i went home, she told me to stop talk...        1.0\n",
              " 4583                              Nemta shook his head.        0.0\n",
              " \n",
              " [4584 rows x 2 columns],\n",
              "                                           SentimentText  Sentiment\n",
              " 0                                   I'm just a dumbass.        1.0\n",
              " 1     I feel like death is flirting with me, always ...        1.0\n",
              " 2     I am now realizing how much I rely on drugs, n...        1.0\n",
              " 3     Submerging, the sharing of emotions, thoughts ...        0.0\n",
              " 4     I feel like even at my most depressed I never ...        1.0\n",
              " ...                                                 ...        ...\n",
              " 1142  Your conscience can try to stop them but its o...        0.0\n",
              " 1143                               The door was locked.        0.0\n",
              " 1144                     it’s just so easy to not care.        0.0\n",
              " 1145  He'd been a fool to think he could survive on ...        0.0\n",
              " 1146  The last world war between NATO and the combin...        0.0\n",
              " \n",
              " [1147 rows x 2 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhw7JuROx4k0",
        "colab_type": "code",
        "outputId": "9370300c-f669-4e21-8fbe-2128564156d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4584, 2), (1147, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4HH5FC3x4k4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('/content/drive/My Drive/depressionrnn/train_tweets.csv', index=False)\n",
        "test.to_csv('/content/drive/My Drive/depressionrnn/test_tweets.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wr075uWx4k-",
        "colab_type": "text"
      },
      "source": [
        "#### defining a funtion to clean the tweets by removing non alphanumeric character and links "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be72LhxIx4k_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tweet_clean(text):\n",
        "    \n",
        "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) \n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text) \n",
        "    \n",
        "    return text.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDFTBXkux4lF",
        "colab_type": "text"
      },
      "source": [
        "####  The tweet column (‘SentimentText’) needs processing and tokenization, so that it can be converted into indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2bc5C4Ux4lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "def tokenizer(s): \n",
        "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zgZS_Hhx4lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize = tokenizer)\n",
        "\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypKAesKgx4lL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datafields = [('SentimentText', TEXT),('Sentiment', LABEL)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA7pIHUJx4lO",
        "colab_type": "text"
      },
      "source": [
        "#### We create torchtext dataset,TabularDataset which is specially designed to read csv and tsv files and process them. It is a wrapper around pytorch Dataset with additional features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHLL6FZqx4lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn, tst = torchtext.data.TabularDataset.splits(path = '/content/drive/My Drive/depressionrnn', \n",
        "                                                train = 'train_tweets.csv',\n",
        "                                                test = 'test_tweets.csv',    \n",
        "                                                format = 'csv',\n",
        "                                                skip_header = True,\n",
        "                                                fields = datafields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5JKc5Oex4lR",
        "colab_type": "code",
        "outputId": "5bba714f-14b7-46ce-80f8-e72cf4d17e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f'Number of training examples: {len(trn)}')\n",
        "print(f'Number of testing examples: {len(tst)}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 4584\n",
            "Number of testing examples: 1147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6p-X9lox4lU",
        "colab_type": "code",
        "outputId": "3217c580-e2c6-43e4-8167-1267f8e2c6f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "vars(trn.examples[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': '0.0',\n",
              " 'SentimentText': ['i',\n",
              "  'keep',\n",
              "  'trying',\n",
              "  'and',\n",
              "  'scrambling',\n",
              "  'to',\n",
              "  'pull',\n",
              "  'myself',\n",
              "  'out',\n",
              "  'of',\n",
              "  'this',\n",
              "  'pit',\n",
              "  'but',\n",
              "  'as',\n",
              "  'soon',\n",
              "  'as',\n",
              "  'i',\n",
              "  'm',\n",
              "  'almost',\n",
              "  'out',\n",
              "  'i',\n",
              "  'get',\n",
              "  'slammed',\n",
              "  'into',\n",
              "  'it',\n",
              "  'again']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeR0dYB6x4lX",
        "colab_type": "code",
        "outputId": "9ea0f40e-dcad-4a56-ff78-7d4c6ba4bf7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vars(tst.examples[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': '1.0', 'SentimentText': ['i', 'm', 'just', 'a', 'dumbass']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mm1hzZnx4lb",
        "colab_type": "text"
      },
      "source": [
        "#### Load pretrained word vectors and build vocabulary\n",
        "Now, instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors. We get these vectors simply by specifying which vectors we want and passing it as an argument to build_vocab. TorchText handles downloading the vectors and associating them with the correct words in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIwIxsI-x4lb",
        "colab_type": "code",
        "outputId": "f70c3b2d-4311-46e8-9df5-473665d0743c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "TEXT.build_vocab(trn, max_size=25000,\n",
        "                 vectors=\"glove.6B.100d\",\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(trn)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                           \n",
            "100%|█████████▉| 398892/400000 [00:17<00:00, 23169.93it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Vh_L2Kx4le",
        "colab_type": "code",
        "outputId": "d73fd9f8-e195-4e3d-ade4-bfd8f47a8173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(50))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('i', 4137), ('the', 2439), ('to', 2270), ('and', 2018), ('a', 1459), ('of', 1169), ('my', 1163), ('it', 1091), ('that', 904), ('in', 847), ('t', 807), ('me', 730), ('was', 698), ('but', 638), ('for', 586), ('m', 579), ('is', 567), ('this', 542), ('you', 528), ('s', 520), ('with', 512), ('have', 493), ('just', 488), ('so', 468), ('on', 430), ('like', 428), ('not', 423), ('he', 417), ('be', 405), ('she', 388), ('at', 374), ('her', 358), ('they', 356), ('do', 351), ('as', 333), ('all', 316), ('feel', 310), ('can', 309), ('don', 309), ('if', 297), ('had', 294), ('what', 283), ('about', 269), ('we', 262), ('know', 251), ('or', 249), ('up', 246), ('one', 246), ('out', 241), ('get', 240)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auLV18Hbx4lh",
        "colab_type": "code",
        "outputId": "a5f4af1d-46a6-4353-d23f-ae1e7fee5f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'i', 'the', 'to', 'and', 'a', 'of', 'my', 'it']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDW7iqfVx4lk",
        "colab_type": "code",
        "outputId": "deff78d4-d118-4258-b6f5-44c01862cad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f79dbb3da60>, {'0.0': 0, '1.0': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hSgCTttx4ln",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the data in batches\n",
        "For data with variable length sentences torchtext provides BucketIterator() dataloader which is wrapper around pytorch Dataloader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3iQxT-lx4lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "                                (trn, tst),\n",
        "                                batch_size = 64,\n",
        "                                sort_key=lambda x: len(x.SentimentText),\n",
        "                                sort_within_batch=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dahjcq_gx4lq",
        "colab_type": "text"
      },
      "source": [
        "#### We'll be using a different RNN architecture called a Long Short-Term Memory (LSTM).\n",
        "\n",
        "<b>torch.nn.embedding</b> -A simple lookup table that stores embeddings of a fixed dictionary and size.This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
        "\n",
        "\n",
        "<b>LSTM</b> - Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
        "\n",
        "<b>bidirectional</b> - an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the last to the first (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$.\n",
        "\n",
        "\n",
        "<b>Dropout</b> - it works by randomly dropping out (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently.This helps in regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwvBK0Z3x4lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
        "                 output_dim, n_layers, bidirectional, dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
        "                           bidirectional = bidirectional, dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "       \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVvtaE2hx4lu",
        "colab_type": "text"
      },
      "source": [
        "To ensure the pre-trained vectors can be loaded into the model, the EMBEDDING_DIM must be equal to that of the pre-trained GloVe vectors loaded earlier.\n",
        "\n",
        "We get our pad token index from the vocabulary, getting the actual string representing the pad token from the field's pad_token attribute, which is pad by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgN2x9qHx4lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "\n",
        "dropout = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYo5rlbTaJIb",
        "colab_type": "code",
        "outputId": "6b73b46d-0b6e-4101-94cf-106584952be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_dim"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8038"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a1W_19xx4ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(input_dim, \n",
        "            embedding_dim, \n",
        "            hidden_dim, \n",
        "            output_dim, \n",
        "            n_layers, \n",
        "            bidirectional, \n",
        "            dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSM3SFzKx4l1",
        "colab_type": "code",
        "outputId": "0e9128d0-3e3d-494a-d546-936ce4e3b298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(8038, 100)\n",
              "  (rnn): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVuF1-Ex4l4",
        "colab_type": "text"
      },
      "source": [
        "We retrieve the embeddings from the field's vocab, and check they're the correct size, [vocab size, embedding dim]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdiJ1iWSx4l4",
        "colab_type": "code",
        "outputId": "045d4c77-9108-46e2-8c90-98e2d7005933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8038, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0m6qBQ-x4l7",
        "colab_type": "text"
      },
      "source": [
        "We then replace the initial weights of the embedding layer with the pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9QJVwAOx4l8",
        "colab_type": "code",
        "outputId": "c73c2cc5-b669-4979-98cb-e89418fbe941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4096, -2.0166, -0.1875,  ...,  0.5066,  1.1730,  1.9223],\n",
              "        [ 0.8996, -1.5762, -0.0165,  ..., -0.1365,  2.3520,  1.8077],\n",
              "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
              "        ...,\n",
              "        [-0.5879,  0.0165, -0.6412,  ...,  0.1248,  0.0362, -0.3741],\n",
              "        [ 0.5732, -1.0756, -0.1600,  ...,  0.4548,  0.2344,  0.0364],\n",
              "        [-0.2909, -0.7498, -0.2997,  ..., -0.4769,  0.4399, -0.2560]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDxVFgKgx4mA",
        "colab_type": "text"
      },
      "source": [
        "As our < unk > and < pad > token aren't in the pre-trained vocabulary they have been initialized using unk_init (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLrRP64wx4mA",
        "colab_type": "code",
        "outputId": "b079a6c0-3b1c-4bab-89c4-4a93a8b1d3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
            "        ...,\n",
            "        [-0.5879,  0.0165, -0.6412,  ...,  0.1248,  0.0362, -0.3741],\n",
            "        [ 0.5732, -1.0756, -0.1600,  ...,  0.4548,  0.2344,  0.0364],\n",
            "        [-0.2909, -0.7498, -0.2997,  ..., -0.4769,  0.4399, -0.2560]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-hoeQfCx4mD",
        "colab_type": "text"
      },
      "source": [
        "#### Train the Model\n",
        "\n",
        "We use Adam optimizer and loss function is BCEWithLogitLoss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PkBLvDzx4mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.0005)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ9CWMN0x4mJ",
        "colab_type": "text"
      },
      "source": [
        "#### We define a function for training our model\n",
        "as we are now using dropout, we must remember to use model.train() to ensure the dropout is \"turned on\" while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2SzsrOOx4mJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.SentimentText).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.Sentiment)\n",
        "        \n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct = (rounded_preds == batch.Sentiment).float() \n",
        "        \n",
        "        acc = correct.sum() / len(correct)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVPNCIGUx4mN",
        "colab_type": "code",
        "outputId": "16c2eb36-5031-4dbf-c50d-e0f30741736a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "num_epochs = 18\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "     \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in test_iterator:\n",
        "\n",
        "            predictions = model(batch.SentimentText).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.Sentiment)\n",
        "\n",
        "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "            correct = (rounded_preds == batch.Sentiment).float() \n",
        "            \n",
        "            acc = correct.sum()/len(correct)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "    test_loss = epoch_loss / len(test_iterator)\n",
        "    test_acc = epoch_acc / len(test_iterator)\n",
        "\n",
        "    \n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')\n",
        "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.648 | Train Acc: 64.76% |\n",
            "Test Loss: 0.628 | Test Acc: 67.79%\n",
            "| Epoch: 02 | Train Loss: 0.616 | Train Acc: 68.19% |\n",
            "Test Loss: 0.613 | Test Acc: 68.23%\n",
            "| Epoch: 03 | Train Loss: 0.593 | Train Acc: 68.28% |\n",
            "Test Loss: 0.611 | Test Acc: 64.99%\n",
            "| Epoch: 04 | Train Loss: 0.536 | Train Acc: 71.67% |\n",
            "Test Loss: 0.604 | Test Acc: 66.76%\n",
            "| Epoch: 05 | Train Loss: 0.515 | Train Acc: 73.97% |\n",
            "Test Loss: 0.634 | Test Acc: 64.77%\n",
            "| Epoch: 06 | Train Loss: 0.502 | Train Acc: 74.75% |\n",
            "Test Loss: 0.616 | Test Acc: 65.90%\n",
            "| Epoch: 07 | Train Loss: 0.482 | Train Acc: 75.49% |\n",
            "Test Loss: 0.667 | Test Acc: 62.85%\n",
            "| Epoch: 08 | Train Loss: 0.471 | Train Acc: 77.03% |\n",
            "Test Loss: 0.586 | Test Acc: 68.95%\n",
            "| Epoch: 09 | Train Loss: 0.447 | Train Acc: 78.82% |\n",
            "Test Loss: 0.580 | Test Acc: 69.39%\n",
            "| Epoch: 10 | Train Loss: 0.449 | Train Acc: 79.01% |\n",
            "Test Loss: 0.560 | Test Acc: 71.47%\n",
            "| Epoch: 11 | Train Loss: 0.434 | Train Acc: 79.80% |\n",
            "Test Loss: 0.584 | Test Acc: 69.12%\n",
            "| Epoch: 12 | Train Loss: 0.422 | Train Acc: 80.81% |\n",
            "Test Loss: 0.586 | Test Acc: 69.73%\n",
            "| Epoch: 13 | Train Loss: 0.410 | Train Acc: 81.23% |\n",
            "Test Loss: 0.502 | Test Acc: 76.33%\n",
            "| Epoch: 14 | Train Loss: 0.409 | Train Acc: 82.08% |\n",
            "Test Loss: 0.525 | Test Acc: 74.58%\n",
            "| Epoch: 15 | Train Loss: 0.398 | Train Acc: 82.22% |\n",
            "Test Loss: 0.553 | Test Acc: 73.28%\n",
            "| Epoch: 16 | Train Loss: 0.384 | Train Acc: 83.05% |\n",
            "Test Loss: 0.527 | Test Acc: 74.41%\n",
            "| Epoch: 17 | Train Loss: 0.385 | Train Acc: 82.43% |\n",
            "Test Loss: 0.532 | Test Acc: 73.97%\n",
            "| Epoch: 18 | Train Loss: 0.361 | Train Acc: 84.25% |\n",
            "Test Loss: 0.511 | Test Acc: 76.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6MfHUjEx4mR",
        "colab_type": "text"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfuzS1z6x4mS",
        "colab_type": "code",
        "outputId": "232dabec-a65d-4980-f369-a75a26d9dfb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        predictions = model(batch.SentimentText).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.Sentiment)\n",
        "\n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct = (rounded_preds == batch.Sentiment).float() \n",
        "        \n",
        "        acc = correct.sum()/len(correct)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "test_loss = epoch_loss / len(test_iterator)\n",
        "test_acc = epoch_acc / len(test_iterator)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.511 | Test Acc: 76.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RX_savqx4mV",
        "colab_type": "text"
      },
      "source": [
        "#### User Input\n",
        "We can now use our model to predict the sentiment of any sentence we give it.As it has been trained on tweets, the sentences provided should in a positive or a negative context.\n",
        "\n",
        "We are expecting tweets with a negative sentiment to return a value close to 1 and positive tweets to return a value close to 0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8juRfr9x4mV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jvsMXlgx4mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okpg-GWEx4mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexed = [TEXT.vocab.stoi[t] for t in tokenized]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y22hVPzdx4mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor = torch.LongTensor(indexed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ijxjMkjx4mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor = tensor.unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37TEQqXxx4ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = torch.sigmoid(model(tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFHYlXEYx4mo",
        "colab_type": "code",
        "outputId": "aef3a0e2-9f43-40b1-9aee-53c118fc7ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prediction.item()"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9267289042472839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryZVfYkYx4ms",
        "colab_type": "code",
        "outputId": "4d0b2d86-f197-4f24-abb7-8f278e1dbc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "torch.save(model, '/content/drive/My Drive/depressionrnn/seventysix.pt')"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLgGIaxVx4mv",
        "colab_type": "code",
        "outputId": "4e97c206-f79b-4a9c-8654-34d89f081a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model = torch.load('/content/drive/My Drive/depressionrnn/eightyALL.pt')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(8038, 100)\n",
              "  (rnn): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNJctbX0oNv2",
        "colab_type": "code",
        "outputId": "da655b97-6c52-4b63-d0ab-cbc2e169b7dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "list(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [-2.5223e-04,  3.6638e-02, -1.2487e-02,  ...,  7.1077e-02,\n",
              "          -6.3295e-03, -4.3267e-02],\n",
              "         [-1.5668e-02,  5.9423e-01,  6.9866e-01,  ..., -4.8228e-01,\n",
              "           4.0013e-02,  8.0493e-01],\n",
              "         ...,\n",
              "         [-7.0798e-01, -1.0008e-01, -7.5247e-01,  ...,  2.3203e-01,\n",
              "          -9.9558e-02, -2.4126e-01],\n",
              "         [ 5.0150e-01, -1.1540e+00, -2.2011e-01,  ...,  5.4036e-01,\n",
              "           2.1950e-01,  1.2718e-01],\n",
              "         [-3.6145e-01, -8.6555e-01, -3.8274e-01,  ..., -5.0392e-01,\n",
              "           4.9332e-01, -1.9343e-01]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.0389,  0.1417,  0.1798,  ..., -0.1074,  0.1154, -0.1111],\n",
              "         [ 0.0304, -0.0568, -0.1228,  ...,  0.1031,  0.2214, -0.0489],\n",
              "         [-0.1973,  0.0014,  0.1398,  ...,  0.1347,  0.0105, -0.0006],\n",
              "         ...,\n",
              "         [-0.1481, -0.0159,  0.2606,  ...,  0.1536, -0.1178, -0.0721],\n",
              "         [ 0.0489, -0.1487,  0.1835,  ..., -0.1672, -0.1767, -0.0013],\n",
              "         [ 0.2594,  0.0608,  0.1482,  ...,  0.1884, -0.1939,  0.0280]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0370,  0.0092, -0.2192,  ..., -0.1160,  0.1792, -0.0550],\n",
              "         [-0.2595, -0.2041, -0.3113,  ...,  0.0485,  0.1925, -0.1656],\n",
              "         [-0.3452, -0.1886,  0.0037,  ..., -0.0512,  0.2115, -0.0836],\n",
              "         ...,\n",
              "         [ 0.0822, -0.1759,  0.0744,  ..., -0.0906, -0.0812,  0.0410],\n",
              "         [-0.1433, -0.2440, -0.0131,  ..., -0.1730,  0.0230,  0.0536],\n",
              "         [ 0.0247,  0.1282, -0.1340,  ..., -0.2507,  0.1730, -0.1437]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.1092, -0.0853,  0.0167, -0.0683,  0.0955,  0.1128, -0.0419, -0.1580,\n",
              "         -0.2288, -0.0386,  0.3133,  0.2113, -0.0712,  0.1161, -0.0191,  0.0711,\n",
              "          0.0960, -0.1584,  0.2660, -0.0288,  0.0945, -0.0538,  0.2328,  0.2147,\n",
              "          0.1992,  0.2023,  0.3354,  0.1276, -0.0377,  0.2505, -0.1461,  0.0372,\n",
              "          0.1582,  0.1056,  0.2770, -0.1247, -0.1226,  0.1377,  0.2475,  0.0284,\n",
              "         -0.0014,  0.0870,  0.1704, -0.2032,  0.1989,  0.1120, -0.1040,  0.0497,\n",
              "         -0.2411, -0.1957, -0.0296, -0.1737,  0.1113, -0.1635, -0.0304,  0.0372,\n",
              "          0.1359,  0.0224,  0.0045,  0.0899], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.1765,  0.1031,  0.2814, -0.1422,  0.0082,  0.0303,  0.1491,  0.0889,\n",
              "         -0.1493, -0.0699,  0.0638, -0.0392,  0.1407,  0.0068, -0.1769,  0.1349,\n",
              "         -0.0939, -0.2079, -0.0767, -0.1815,  0.2369,  0.1442, -0.0708,  0.0932,\n",
              "          0.1479, -0.0568,  0.1630,  0.0901,  0.0406,  0.1373, -0.0037, -0.0681,\n",
              "          0.0329, -0.0575,  0.1975,  0.0679,  0.2642,  0.1199, -0.0404, -0.0931,\n",
              "          0.1789, -0.0779,  0.0531, -0.0099, -0.1093, -0.1055, -0.0392,  0.0727,\n",
              "         -0.1160,  0.0418,  0.1909, -0.0334, -0.1492,  0.0561, -0.1050,  0.0459,\n",
              "          0.1622, -0.1406, -0.1681,  0.1602], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.0345, -0.1273,  0.0242,  ..., -0.2223,  0.1683,  0.3636],\n",
              "         [-0.0741,  0.0612,  0.1508,  ..., -0.2246,  0.2265, -0.0924],\n",
              "         [-0.2788, -0.2100,  0.1099,  ...,  0.2655, -0.0052,  0.2326],\n",
              "         ...,\n",
              "         [-0.1915, -0.2346, -0.0506,  ...,  0.0018,  0.0866,  0.2077],\n",
              "         [ 0.0544, -0.1274,  0.1708,  ..., -0.2088, -0.0447, -0.0243],\n",
              "         [ 0.1683,  0.0554,  0.1518,  ...,  0.2039, -0.1493, -0.2608]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0848, -0.1065, -0.0358,  ...,  0.1418,  0.1808,  0.1280],\n",
              "         [ 0.0738,  0.0008, -0.1148,  ..., -0.1629,  0.0546,  0.0681],\n",
              "         [ 0.0616, -0.2231, -0.0281,  ...,  0.0718,  0.1336,  0.0054],\n",
              "         ...,\n",
              "         [-0.2435,  0.2941, -0.1565,  ...,  0.1983,  0.0626,  0.0624],\n",
              "         [-0.0465,  0.1405, -0.1880,  ...,  0.1591,  0.2728, -0.2961],\n",
              "         [ 0.0725,  0.1467,  0.0978,  ..., -0.1992, -0.0257,  0.1341]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0755,  0.1317, -0.0658, -0.2054, -0.1199, -0.1839,  0.1251, -0.2368,\n",
              "         -0.0952, -0.0893, -0.2556,  0.2327, -0.0365,  0.1240,  0.0857,  0.2116,\n",
              "         -0.0649,  0.0959,  0.1782, -0.1883, -0.0302, -0.0603, -0.0343, -0.0431,\n",
              "         -0.0713,  0.1266, -0.1893, -0.0576, -0.0213, -0.1548,  0.1252,  0.1328,\n",
              "          0.0599,  0.1824,  0.1219, -0.1983,  0.1537, -0.1678,  0.3554, -0.0517,\n",
              "          0.0222, -0.1314,  0.1465, -0.1479,  0.1992,  0.0088, -0.1035,  0.0778,\n",
              "          0.2187, -0.0158, -0.1297, -0.1304, -0.1746,  0.0886, -0.1536, -0.0710,\n",
              "          0.0499, -0.0305,  0.0699, -0.2039], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0451, -0.0524,  0.0964, -0.2474, -0.0969,  0.2199,  0.0720,  0.0497,\n",
              "         -0.1367, -0.0845, -0.1155, -0.1227,  0.0766, -0.0061, -0.2833,  0.3210,\n",
              "          0.0959, -0.0334,  0.0811,  0.0059, -0.0318, -0.1398, -0.1800, -0.0187,\n",
              "          0.1482, -0.1973, -0.0679, -0.1925, -0.0794,  0.0598, -0.1475, -0.0596,\n",
              "          0.1224,  0.0264, -0.1338, -0.0090, -0.1234, -0.2008, -0.0587,  0.1047,\n",
              "          0.1912,  0.1235,  0.2133, -0.1900,  0.1256, -0.2184, -0.1998,  0.0493,\n",
              "          0.0570,  0.1107,  0.1473,  0.1202, -0.0867, -0.0499, -0.1643,  0.2697,\n",
              "         -0.0965, -0.1922,  0.2157,  0.0642], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0824,  0.1616,  0.2379,  ...,  0.0805,  0.0915, -0.1189],\n",
              "         [ 0.2467, -0.0936, -0.2160,  ...,  0.1682,  0.0243,  0.2638],\n",
              "         [ 0.1720,  0.0525,  0.0512,  ..., -0.2222, -0.0324, -0.1584],\n",
              "         ...,\n",
              "         [ 0.0038,  0.1781, -0.1416,  ...,  0.0508,  0.1588, -0.0404],\n",
              "         [-0.2161, -0.1492,  0.0478,  ...,  0.0006,  0.0285, -0.0494],\n",
              "         [ 0.1949, -0.1671, -0.1352,  ..., -0.1100,  0.0866, -0.0947]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.1706, -0.1395, -0.1053,  ...,  0.1013, -0.0396, -0.0412],\n",
              "         [-0.2573,  0.0425,  0.0949,  ...,  0.1889,  0.1667, -0.2288],\n",
              "         [ 0.1442,  0.0004,  0.1649,  ..., -0.1528,  0.0336,  0.0711],\n",
              "         ...,\n",
              "         [ 0.0212,  0.2434, -0.1804,  ..., -0.1488, -0.0873, -0.0278],\n",
              "         [ 0.0373,  0.2201,  0.1262,  ...,  0.1244,  0.1594, -0.0867],\n",
              "         [-0.1714, -0.0110, -0.2178,  ...,  0.0523, -0.0909,  0.0855]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.0819, -0.0735, -0.0107, -0.0235, -0.1441, -0.1639, -0.1097, -0.0427,\n",
              "         -0.0239, -0.0439, -0.0384, -0.2005,  0.2122,  0.1272,  0.1566, -0.0939,\n",
              "         -0.2597, -0.0730, -0.2006,  0.0098, -0.0466, -0.0807,  0.2943, -0.0060,\n",
              "          0.0199,  0.1094,  0.1633,  0.1654,  0.0025, -0.0510, -0.0973,  0.0471,\n",
              "         -0.0240,  0.0212,  0.1240,  0.0855, -0.0923,  0.1386, -0.2245, -0.0888,\n",
              "          0.2323, -0.0155, -0.1059, -0.1387,  0.0358,  0.0373,  0.0811,  0.2308,\n",
              "         -0.0797,  0.0856,  0.1283, -0.0284,  0.2159, -0.2122, -0.0831, -0.1933,\n",
              "         -0.1517, -0.1263, -0.1674,  0.1851], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.0764, -0.0976,  0.0251, -0.1554, -0.1438,  0.1572, -0.0197, -0.1596,\n",
              "         -0.1797,  0.0848, -0.1933, -0.0829,  0.0660, -0.1371,  0.1265, -0.0149,\n",
              "          0.0439, -0.1575, -0.0355, -0.1456,  0.0387,  0.0203,  0.0045, -0.2018,\n",
              "          0.2222,  0.1369,  0.0987,  0.0774, -0.1689, -0.1574,  0.0949,  0.2254,\n",
              "         -0.0805,  0.1226, -0.0163, -0.0555, -0.0965, -0.0532,  0.0012, -0.1882,\n",
              "         -0.1703, -0.0976,  0.0318, -0.0217,  0.1192, -0.2184, -0.0778, -0.0209,\n",
              "          0.0406,  0.0835,  0.0320, -0.0653,  0.2201, -0.1435, -0.0533,  0.0532,\n",
              "          0.0294, -0.0145, -0.1742, -0.1683], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.2240, -0.0213,  0.3014,  ..., -0.0480,  0.1117,  0.1653],\n",
              "         [ 0.0631,  0.2167, -0.0242,  ..., -0.1658,  0.0586, -0.1832],\n",
              "         [ 0.1625, -0.2596,  0.0647,  ..., -0.1069, -0.0580, -0.0342],\n",
              "         ...,\n",
              "         [-0.1451,  0.0899, -0.2068,  ..., -0.0400, -0.0400, -0.0665],\n",
              "         [ 0.0231,  0.0258, -0.2611,  ..., -0.1361,  0.0995,  0.0730],\n",
              "         [-0.1254,  0.1295,  0.0997,  ...,  0.1250,  0.1869, -0.0466]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.2382,  0.2001, -0.1299,  ...,  0.1070,  0.0974, -0.1032],\n",
              "         [-0.2444,  0.0534,  0.0883,  ..., -0.1597, -0.0699,  0.0408],\n",
              "         [-0.2951, -0.0148,  0.0745,  ...,  0.1347,  0.0533,  0.2025],\n",
              "         ...,\n",
              "         [ 0.3568,  0.0441,  0.1534,  ..., -0.1318,  0.1226,  0.0073],\n",
              "         [ 0.3055,  0.0309,  0.0858,  ..., -0.0537, -0.1627, -0.0829],\n",
              "         [-0.0437,  0.0275,  0.0008,  ...,  0.0234,  0.1512, -0.0465]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0300,  0.1190, -0.0244,  0.1574,  0.0788,  0.1869,  0.2107, -0.0745,\n",
              "          0.0488,  0.3065, -0.1614,  0.2159,  0.1114,  0.1757,  0.0683,  0.0458,\n",
              "         -0.1176, -0.0131,  0.3202,  0.1262,  0.3253,  0.0319,  0.1522, -0.2531,\n",
              "         -0.0043,  0.0951,  0.2826,  0.0861, -0.1120,  0.0326, -0.2646,  0.2595,\n",
              "          0.2253, -0.0103, -0.1204,  0.1159,  0.1395,  0.1903, -0.0624,  0.0296,\n",
              "         -0.1989, -0.0679, -0.2154,  0.1410, -0.0283, -0.1359, -0.0375, -0.2060,\n",
              "         -0.0509, -0.2080, -0.0518, -0.0526, -0.1044,  0.1664,  0.0439, -0.1204,\n",
              "         -0.1291,  0.1299, -0.0612,  0.2313], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0421, -0.0562,  0.1696,  0.0542, -0.0653,  0.1156,  0.2030, -0.1242,\n",
              "         -0.0888,  0.2254,  0.1522,  0.2264, -0.1402, -0.0349,  0.2185,  0.2772,\n",
              "          0.1082,  0.1431,  0.0789, -0.1210,  0.0686, -0.1889, -0.0615, -0.1741,\n",
              "         -0.1044,  0.1806,  0.1219, -0.1497, -0.0990,  0.2991, -0.0472,  0.0677,\n",
              "         -0.1096,  0.2154,  0.0493,  0.1822,  0.1141,  0.0967, -0.0098,  0.1082,\n",
              "         -0.2239,  0.1407,  0.0424,  0.1157, -0.1228, -0.1399, -0.0110,  0.1777,\n",
              "          0.1704, -0.1438,  0.1065, -0.1201,  0.1077, -0.1760, -0.0939, -0.0488,\n",
              "          0.2586, -0.0624,  0.0948,  0.2131], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.1133, -0.0862, -0.1000,  0.0349, -0.0461,  0.1092, -0.0783, -0.0322,\n",
              "           0.1008,  0.0622, -0.0480,  0.0409, -0.1083,  0.1198,  0.0738,  0.0501,\n",
              "           0.1381,  0.0560,  0.0928, -0.0513,  0.3745, -0.2902,  0.2743,  0.1810,\n",
              "          -0.3280,  0.2839,  0.2822,  0.3185, -0.3272,  0.2721, -0.2547, -0.3385,\n",
              "          -0.1911, -0.2523,  0.2676, -0.2414, -0.3289,  0.3301,  0.2295, -0.3569]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.0730], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fOLNZ3O03iq",
        "colab_type": "text"
      },
      "source": [
        "Now We gotta build the second RNN to understand the relations between sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of2NWhpGoUbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fovP7kTc54RZ",
        "colab_type": "code",
        "outputId": "bfec4462-cbc4-4ca5-b2e4-afdf6cb75108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zovsnOFx7RWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "para = pd.read_csv('/content/drive/My Drive/depressionrnn/paradataset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlEC8MNx8eFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lmao=para[0:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv9taXVl9fy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = {'condition':[]}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV9MrFNJ7hln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(para)):\n",
        "  input_data['condition'].append(para.loc[i, \"condition\"])\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYf6ie_qe9if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_mer= pd.DataFrame(input_data,columns =['condition'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDjViLZ-vpeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=0\n",
        "m=0\n",
        "for i in range(len(para)):\n",
        "  sent_tokens = sent_tokenize(para.loc[i, \"submission\"])\n",
        "  l=l+len(sent_tokens)\n",
        "m=l/i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IApgYdr2v8RB",
        "colab_type": "code",
        "outputId": "0d87053f-0745-4001-9233-132abed4db8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "m"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35.506864988558355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 633
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cm_Mrryxhkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_sentence = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NPtFU5Ax7JH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(para)):\n",
        "  sent_tokens = sent_tokenize(para.loc[i, \"submission\"])\n",
        "  for i in range(len(sent_tokens)):\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sent_tokens[i])]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    vec_sentence.setdefault(i,[]).append(prediction.item())\n",
        "  if i<580:\n",
        "    for i in range(i+1,580):\n",
        "      vec_sentence.setdefault(i,[]).append(0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr_S5aPA2gLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_sent = pd.DataFrame(vec_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSxRVg-d9saQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_vec_sent=vec_sent.iloc[:,0:36]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEiiye0vA67K",
        "colab_type": "code",
        "outputId": "5c04bfd1-24f0-4d3b-9b14-0be509fe81ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "average_vec_sent.sample(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>0.807537</td>\n",
              "      <td>0.943191</td>\n",
              "      <td>0.009136</td>\n",
              "      <td>0.002906</td>\n",
              "      <td>0.962754</td>\n",
              "      <td>0.004715</td>\n",
              "      <td>0.008831</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1711</th>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.003030</td>\n",
              "      <td>0.016530</td>\n",
              "      <td>0.001273</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.351126</td>\n",
              "      <td>0.001891</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.022999</td>\n",
              "      <td>0.015549</td>\n",
              "      <td>0.024722</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.034086</td>\n",
              "      <td>0.00641</td>\n",
              "      <td>0.014226</td>\n",
              "      <td>0.013526</td>\n",
              "      <td>0.004714</td>\n",
              "      <td>0.02879</td>\n",
              "      <td>0.374671</td>\n",
              "      <td>0.961756</td>\n",
              "      <td>0.419337</td>\n",
              "      <td>0.71161</td>\n",
              "      <td>0.000983</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2         3         4   ...   31   32   33   34   35\n",
              "335   0.807537  0.943191  0.009136  0.002906  0.962754  ...  0.0  0.0  0.0  0.0  0.0\n",
              "1711  0.000331  0.001359  0.003030  0.016530  0.001273  ...  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[2 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 638
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFzjZiIAxawS",
        "colab_type": "text"
      },
      "source": [
        "mean value of number of sentences = 36"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sCA7beq_0Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_para_vectors = pd.DataFrame.from_dict(input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woa7LHd-1RHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens = sent_tokenize(\"\") \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otzfui2-1cWE",
        "colab_type": "code",
        "outputId": "e25a1915-4695-4a02-e6ae-4427bc777eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "input_data = []\n",
        "sent_tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i am fed up with my life.',\n",
              " 'There is nothing much left.',\n",
              " 'I want to die.',\n",
              " 'i like taking  walks by the  beach']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 366
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG7UZmyF6ASw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sentence in sent_tokens:\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  tensor = torch.LongTensor(indexed)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  prediction = torch.sigmoid(model(tensor))\n",
        "  input_data.append(prediction.item())\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4t0Lj7W6qr1",
        "colab_type": "code",
        "outputId": "38a37919-2089-4871-d437-a64585fec87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "input_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9255887269973755,\n",
              " 0.8959446549415588,\n",
              " 0.9133660793304443,\n",
              " 0.11528420448303223]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 368
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0wDQQLZD_Cw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAxAyaUnBrox",
        "colab_type": "code",
        "outputId": "9009f268-d2e8-4a04-962f-d403e0b2afba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "data_para_vectors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.023430433124303818, 0.10041609406471252, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.7287686467170715, 0.13594579696655273, 0.07...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.019319090992212296, 0.8115354180335999, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.9652117490768433, 0.9601704478263855, 0.974...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.01882638968527317, 0.017171267420053482, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>[0.02131967432796955, 0.021098053082823753, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>[0.021219639107584953, 0.0192734282463789, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>[0.18058808147907257, 0.8963363170623779, 0.93...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>[0.2600974440574646, 0.018560275435447693, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>[0.8456486463546753, 0.8619685173034668, 0.920...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1749 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  condition\n",
              "0     [0.023430433124303818, 0.10041609406471252, 0....          0\n",
              "1     [0.7287686467170715, 0.13594579696655273, 0.07...          1\n",
              "2     [0.019319090992212296, 0.8115354180335999, 0.0...          0\n",
              "3     [0.9652117490768433, 0.9601704478263855, 0.974...          1\n",
              "4     [0.01882638968527317, 0.017171267420053482, 0....          0\n",
              "...                                                 ...        ...\n",
              "1744  [0.02131967432796955, 0.021098053082823753, 0....          0\n",
              "1745  [0.021219639107584953, 0.0192734282463789, 0.0...          0\n",
              "1746  [0.18058808147907257, 0.8963363170623779, 0.93...          1\n",
              "1747  [0.2600974440574646, 0.018560275435447693, 0.0...          0\n",
              "1748  [0.8456486463546753, 0.8619685173034668, 0.920...          1\n",
              "\n",
              "[1749 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 405
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNhOVpVPAU_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_para_vectors.to_csv('/content/drive/My Drive/depressionrnn/para_vectors.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCQhqipPBkiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_para_vectors = pd.read_csv('/content/drive/My Drive/depressionrnn/para_vectors.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hsq7EK5KxGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_mer = data_para_vectors.iloc[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXAVapCXBgyS",
        "colab_type": "code",
        "outputId": "95bad97a-cd7c-426a-b015-f23fda48f7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "to_mer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1749 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      condition\n",
              "0             0\n",
              "1             1\n",
              "2             0\n",
              "3             1\n",
              "4             0\n",
              "...         ...\n",
              "1744          0\n",
              "1745          0\n",
              "1746          1\n",
              "1747          0\n",
              "1748          1\n",
              "\n",
              "[1749 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wd15lWiCZ5O",
        "colab_type": "code",
        "outputId": "cd7b2764-3f31-4814-c27e-8f5ccfc13ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "average_vec_sent"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.021249</td>\n",
              "      <td>0.029183</td>\n",
              "      <td>0.009153</td>\n",
              "      <td>0.863360</td>\n",
              "      <td>0.437762</td>\n",
              "      <td>0.027893</td>\n",
              "      <td>0.034607</td>\n",
              "      <td>0.009567</td>\n",
              "      <td>0.539730</td>\n",
              "      <td>0.023512</td>\n",
              "      <td>0.010532</td>\n",
              "      <td>0.073112</td>\n",
              "      <td>0.032779</td>\n",
              "      <td>0.013092</td>\n",
              "      <td>0.015442</td>\n",
              "      <td>0.016392</td>\n",
              "      <td>0.055576</td>\n",
              "      <td>0.015903</td>\n",
              "      <td>0.011706</td>\n",
              "      <td>0.011917</td>\n",
              "      <td>0.014009</td>\n",
              "      <td>0.013470</td>\n",
              "      <td>0.077795</td>\n",
              "      <td>0.016448</td>\n",
              "      <td>0.687319</td>\n",
              "      <td>0.027469</td>\n",
              "      <td>0.062830</td>\n",
              "      <td>0.017083</td>\n",
              "      <td>0.335660</td>\n",
              "      <td>0.133792</td>\n",
              "      <td>0.470579</td>\n",
              "      <td>0.015406</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.034288</td>\n",
              "      <td>0.015089</td>\n",
              "      <td>0.497672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.806606</td>\n",
              "      <td>0.128510</td>\n",
              "      <td>0.136395</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.013921</td>\n",
              "      <td>0.856608</td>\n",
              "      <td>0.015094</td>\n",
              "      <td>0.032395</td>\n",
              "      <td>0.958026</td>\n",
              "      <td>0.903783</td>\n",
              "      <td>0.643045</td>\n",
              "      <td>0.772588</td>\n",
              "      <td>0.973150</td>\n",
              "      <td>0.067712</td>\n",
              "      <td>0.273345</td>\n",
              "      <td>0.818307</td>\n",
              "      <td>0.206419</td>\n",
              "      <td>0.153157</td>\n",
              "      <td>0.766966</td>\n",
              "      <td>0.018340</td>\n",
              "      <td>0.025191</td>\n",
              "      <td>0.009675</td>\n",
              "      <td>0.019293</td>\n",
              "      <td>0.249056</td>\n",
              "      <td>0.024023</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.942060</td>\n",
              "      <td>0.982114</td>\n",
              "      <td>0.983963</td>\n",
              "      <td>0.036931</td>\n",
              "      <td>0.643927</td>\n",
              "      <td>0.967281</td>\n",
              "      <td>0.946222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.013459</td>\n",
              "      <td>0.011142</td>\n",
              "      <td>0.014239</td>\n",
              "      <td>0.014808</td>\n",
              "      <td>0.021258</td>\n",
              "      <td>0.023457</td>\n",
              "      <td>0.014128</td>\n",
              "      <td>0.015280</td>\n",
              "      <td>0.010230</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.015189</td>\n",
              "      <td>0.011412</td>\n",
              "      <td>0.015431</td>\n",
              "      <td>0.017493</td>\n",
              "      <td>0.012073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1742</th>\n",
              "      <td>0.016779</td>\n",
              "      <td>0.016226</td>\n",
              "      <td>0.015499</td>\n",
              "      <td>0.013974</td>\n",
              "      <td>0.715402</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.011302</td>\n",
              "      <td>0.012160</td>\n",
              "      <td>0.965153</td>\n",
              "      <td>0.015847</td>\n",
              "      <td>0.861451</td>\n",
              "      <td>0.021216</td>\n",
              "      <td>0.095369</td>\n",
              "      <td>0.019180</td>\n",
              "      <td>0.010733</td>\n",
              "      <td>0.038995</td>\n",
              "      <td>0.018170</td>\n",
              "      <td>0.019020</td>\n",
              "      <td>0.364901</td>\n",
              "      <td>0.010777</td>\n",
              "      <td>0.024120</td>\n",
              "      <td>0.018600</td>\n",
              "      <td>0.009852</td>\n",
              "      <td>0.026009</td>\n",
              "      <td>0.149471</td>\n",
              "      <td>0.045852</td>\n",
              "      <td>0.017224</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.024373</td>\n",
              "      <td>0.017340</td>\n",
              "      <td>0.150561</td>\n",
              "      <td>0.013599</td>\n",
              "      <td>0.010900</td>\n",
              "      <td>0.013412</td>\n",
              "      <td>0.027110</td>\n",
              "      <td>0.011038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1743</th>\n",
              "      <td>0.014215</td>\n",
              "      <td>0.009749</td>\n",
              "      <td>0.015046</td>\n",
              "      <td>0.024823</td>\n",
              "      <td>0.017891</td>\n",
              "      <td>0.011946</td>\n",
              "      <td>0.011123</td>\n",
              "      <td>0.013225</td>\n",
              "      <td>0.010650</td>\n",
              "      <td>0.013751</td>\n",
              "      <td>0.011576</td>\n",
              "      <td>0.012770</td>\n",
              "      <td>0.010118</td>\n",
              "      <td>0.013741</td>\n",
              "      <td>0.032768</td>\n",
              "      <td>0.251493</td>\n",
              "      <td>0.026829</td>\n",
              "      <td>0.011291</td>\n",
              "      <td>0.009629</td>\n",
              "      <td>0.013361</td>\n",
              "      <td>0.019585</td>\n",
              "      <td>0.109740</td>\n",
              "      <td>0.011663</td>\n",
              "      <td>0.026420</td>\n",
              "      <td>0.016733</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>0.015811</td>\n",
              "      <td>0.016686</td>\n",
              "      <td>0.012995</td>\n",
              "      <td>0.012610</td>\n",
              "      <td>0.019966</td>\n",
              "      <td>0.016812</td>\n",
              "      <td>0.019783</td>\n",
              "      <td>0.013839</td>\n",
              "      <td>0.033524</td>\n",
              "      <td>0.024873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>0.253411</td>\n",
              "      <td>0.627633</td>\n",
              "      <td>0.968265</td>\n",
              "      <td>0.032800</td>\n",
              "      <td>0.768215</td>\n",
              "      <td>0.532675</td>\n",
              "      <td>0.935778</td>\n",
              "      <td>0.015314</td>\n",
              "      <td>0.154825</td>\n",
              "      <td>0.015683</td>\n",
              "      <td>0.014456</td>\n",
              "      <td>0.116663</td>\n",
              "      <td>0.119161</td>\n",
              "      <td>0.099314</td>\n",
              "      <td>0.021685</td>\n",
              "      <td>0.894721</td>\n",
              "      <td>0.019328</td>\n",
              "      <td>0.054429</td>\n",
              "      <td>0.774793</td>\n",
              "      <td>0.811992</td>\n",
              "      <td>0.046720</td>\n",
              "      <td>0.015591</td>\n",
              "      <td>0.013781</td>\n",
              "      <td>0.759258</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.152117</td>\n",
              "      <td>0.928234</td>\n",
              "      <td>0.013049</td>\n",
              "      <td>0.040625</td>\n",
              "      <td>0.031172</td>\n",
              "      <td>0.013982</td>\n",
              "      <td>0.107106</td>\n",
              "      <td>0.074915</td>\n",
              "      <td>0.969812</td>\n",
              "      <td>0.916486</td>\n",
              "      <td>0.016836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>0.343709</td>\n",
              "      <td>0.010765</td>\n",
              "      <td>0.044594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>0.953615</td>\n",
              "      <td>0.950967</td>\n",
              "      <td>0.966567</td>\n",
              "      <td>0.750556</td>\n",
              "      <td>0.900862</td>\n",
              "      <td>0.977851</td>\n",
              "      <td>0.591171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1747 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        33        34        35\n",
              "0     0.021249  0.029183  0.009153  ...  0.034288  0.015089  0.497672\n",
              "1     0.806606  0.128510  0.136395  ...  0.000000  0.000000  0.000000\n",
              "2     0.013921  0.856608  0.015094  ...  0.000000  0.000000  0.000000\n",
              "3     0.942060  0.982114  0.983963  ...  0.000000  0.000000  0.000000\n",
              "4     0.013459  0.011142  0.014239  ...  0.000000  0.000000  0.000000\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "1742  0.016779  0.016226  0.015499  ...  0.013412  0.027110  0.011038\n",
              "1743  0.014215  0.009749  0.015046  ...  0.013839  0.033524  0.024873\n",
              "1744  0.253411  0.627633  0.968265  ...  0.969812  0.916486  0.016836\n",
              "1745  0.343709  0.010765  0.044594  ...  0.000000  0.000000  0.000000\n",
              "1746  0.953615  0.950967  0.966567  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[1747 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENojj7ozC2_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_numeric = pd.merge(to_mer,average_vec_sent,left_index=True,right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogJXGaEsDagM",
        "colab_type": "code",
        "outputId": "1e82a8e2-b81c-4343-bd27-2d6ce52ea64b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "sent_numeric.sample(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>condition</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>1</td>\n",
              "      <td>0.740276</td>\n",
              "      <td>0.022239</td>\n",
              "      <td>0.022524</td>\n",
              "      <td>0.003498</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0</td>\n",
              "      <td>0.170613</td>\n",
              "      <td>0.014809</td>\n",
              "      <td>0.957401</td>\n",
              "      <td>0.740263</td>\n",
              "      <td>0.864061</td>\n",
              "      <td>0.301272</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>1</td>\n",
              "      <td>0.855699</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>1</td>\n",
              "      <td>0.009630</td>\n",
              "      <td>0.065301</td>\n",
              "      <td>0.029075</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.006052</td>\n",
              "      <td>0.011120</td>\n",
              "      <td>0.002487</td>\n",
              "      <td>0.022104</td>\n",
              "      <td>0.011771</td>\n",
              "      <td>0.783413</td>\n",
              "      <td>0.010061</td>\n",
              "      <td>0.002015</td>\n",
              "      <td>0.122728</td>\n",
              "      <td>0.906303</td>\n",
              "      <td>0.227663</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.006626</td>\n",
              "      <td>0.001797</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.029768</td>\n",
              "      <td>0.038406</td>\n",
              "      <td>0.006570</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>1</td>\n",
              "      <td>0.006917</td>\n",
              "      <td>0.002116</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>0.534396</td>\n",
              "      <td>0.005031</td>\n",
              "      <td>0.745344</td>\n",
              "      <td>0.972241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>1</td>\n",
              "      <td>0.000403</td>\n",
              "      <td>0.003856</td>\n",
              "      <td>0.032672</td>\n",
              "      <td>0.010316</td>\n",
              "      <td>0.004237</td>\n",
              "      <td>0.001165</td>\n",
              "      <td>0.020629</td>\n",
              "      <td>0.003802</td>\n",
              "      <td>0.002838</td>\n",
              "      <td>0.027955</td>\n",
              "      <td>0.001272</td>\n",
              "      <td>0.002820</td>\n",
              "      <td>0.013056</td>\n",
              "      <td>0.011720</td>\n",
              "      <td>0.002576</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>0</td>\n",
              "      <td>0.016545</td>\n",
              "      <td>0.269982</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.002475</td>\n",
              "      <td>0.013001</td>\n",
              "      <td>0.004108</td>\n",
              "      <td>0.064349</td>\n",
              "      <td>0.003214</td>\n",
              "      <td>0.003627</td>\n",
              "      <td>0.052068</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.016415</td>\n",
              "      <td>0.018239</td>\n",
              "      <td>0.002740</td>\n",
              "      <td>0.001176</td>\n",
              "      <td>0.011608</td>\n",
              "      <td>0.002122</td>\n",
              "      <td>0.003164</td>\n",
              "      <td>0.004044</td>\n",
              "      <td>0.019285</td>\n",
              "      <td>0.004025</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.003340</td>\n",
              "      <td>0.009337</td>\n",
              "      <td>0.008010</td>\n",
              "      <td>0.051564</td>\n",
              "      <td>0.001838</td>\n",
              "      <td>0.002093</td>\n",
              "      <td>0.006431</td>\n",
              "      <td>0.004131</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>0.013799</td>\n",
              "      <td>0.006962</td>\n",
              "      <td>0.007356</td>\n",
              "      <td>0.003480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>0</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>0.093995</td>\n",
              "      <td>0.102958</td>\n",
              "      <td>0.040028</td>\n",
              "      <td>0.003020</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.003306</td>\n",
              "      <td>0.007483</td>\n",
              "      <td>0.670545</td>\n",
              "      <td>0.205594</td>\n",
              "      <td>0.541328</td>\n",
              "      <td>0.859905</td>\n",
              "      <td>0.012168</td>\n",
              "      <td>0.035211</td>\n",
              "      <td>0.073337</td>\n",
              "      <td>0.001573</td>\n",
              "      <td>0.056919</td>\n",
              "      <td>0.457170</td>\n",
              "      <td>0.001171</td>\n",
              "      <td>0.248678</td>\n",
              "      <td>0.005324</td>\n",
              "      <td>0.056279</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>0</td>\n",
              "      <td>0.006527</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.008283</td>\n",
              "      <td>0.009425</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.510950</td>\n",
              "      <td>0.006309</td>\n",
              "      <td>0.023168</td>\n",
              "      <td>0.002625</td>\n",
              "      <td>0.002894</td>\n",
              "      <td>0.009659</td>\n",
              "      <td>0.001271</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.023453</td>\n",
              "      <td>0.001619</td>\n",
              "      <td>0.003951</td>\n",
              "      <td>0.016812</td>\n",
              "      <td>0.031685</td>\n",
              "      <td>0.012040</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.005749</td>\n",
              "      <td>0.016095</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>0.008620</td>\n",
              "      <td>0.004044</td>\n",
              "      <td>0.005108</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.037876</td>\n",
              "      <td>0.003873</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.008555</td>\n",
              "      <td>0.012211</td>\n",
              "      <td>0.879532</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.003442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>1</td>\n",
              "      <td>0.998414</td>\n",
              "      <td>0.157498</td>\n",
              "      <td>0.075853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>0</td>\n",
              "      <td>0.978252</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>1</td>\n",
              "      <td>0.022945</td>\n",
              "      <td>0.189791</td>\n",
              "      <td>0.018352</td>\n",
              "      <td>0.020695</td>\n",
              "      <td>0.005584</td>\n",
              "      <td>0.003895</td>\n",
              "      <td>0.059578</td>\n",
              "      <td>0.048195</td>\n",
              "      <td>0.927239</td>\n",
              "      <td>0.017428</td>\n",
              "      <td>0.646797</td>\n",
              "      <td>0.103097</td>\n",
              "      <td>0.152126</td>\n",
              "      <td>0.776056</td>\n",
              "      <td>0.051708</td>\n",
              "      <td>0.283725</td>\n",
              "      <td>0.010864</td>\n",
              "      <td>0.028482</td>\n",
              "      <td>0.136096</td>\n",
              "      <td>0.062807</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1102</th>\n",
              "      <td>1</td>\n",
              "      <td>0.456257</td>\n",
              "      <td>0.007609</td>\n",
              "      <td>0.359241</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769</th>\n",
              "      <td>1</td>\n",
              "      <td>0.975572</td>\n",
              "      <td>0.986297</td>\n",
              "      <td>0.991856</td>\n",
              "      <td>0.382659</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>1</td>\n",
              "      <td>0.041337</td>\n",
              "      <td>0.007085</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.005382</td>\n",
              "      <td>0.002170</td>\n",
              "      <td>0.009503</td>\n",
              "      <td>0.008707</td>\n",
              "      <td>0.002104</td>\n",
              "      <td>0.001381</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.002042</td>\n",
              "      <td>0.002449</td>\n",
              "      <td>0.205966</td>\n",
              "      <td>0.003249</td>\n",
              "      <td>0.004465</td>\n",
              "      <td>0.003796</td>\n",
              "      <td>0.003194</td>\n",
              "      <td>0.111441</td>\n",
              "      <td>0.091992</td>\n",
              "      <td>0.003866</td>\n",
              "      <td>0.035003</td>\n",
              "      <td>0.001287</td>\n",
              "      <td>0.068460</td>\n",
              "      <td>0.063670</td>\n",
              "      <td>0.010557</td>\n",
              "      <td>0.048925</td>\n",
              "      <td>0.186068</td>\n",
              "      <td>0.047546</td>\n",
              "      <td>0.018690</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>0.001377</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.011010</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.032095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>0</td>\n",
              "      <td>0.012352</td>\n",
              "      <td>0.013663</td>\n",
              "      <td>0.016347</td>\n",
              "      <td>0.023008</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>0.010503</td>\n",
              "      <td>0.011117</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1049</th>\n",
              "      <td>0</td>\n",
              "      <td>0.001847</td>\n",
              "      <td>0.016920</td>\n",
              "      <td>0.009703</td>\n",
              "      <td>0.018654</td>\n",
              "      <td>0.091507</td>\n",
              "      <td>0.015029</td>\n",
              "      <td>0.407468</td>\n",
              "      <td>0.048514</td>\n",
              "      <td>0.000471</td>\n",
              "      <td>0.551244</td>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.015071</td>\n",
              "      <td>0.010394</td>\n",
              "      <td>0.003183</td>\n",
              "      <td>0.003220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>1</td>\n",
              "      <td>0.040182</td>\n",
              "      <td>0.179082</td>\n",
              "      <td>0.325193</td>\n",
              "      <td>0.062333</td>\n",
              "      <td>0.001061</td>\n",
              "      <td>0.004674</td>\n",
              "      <td>0.019299</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>0.000928</td>\n",
              "      <td>0.790561</td>\n",
              "      <td>0.907686</td>\n",
              "      <td>0.004375</td>\n",
              "      <td>0.016123</td>\n",
              "      <td>0.450490</td>\n",
              "      <td>0.414101</td>\n",
              "      <td>0.069870</td>\n",
              "      <td>0.004706</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.001677</td>\n",
              "      <td>0.002115</td>\n",
              "      <td>0.063940</td>\n",
              "      <td>0.088509</td>\n",
              "      <td>0.004641</td>\n",
              "      <td>0.010903</td>\n",
              "      <td>0.007687</td>\n",
              "      <td>0.600169</td>\n",
              "      <td>0.296294</td>\n",
              "      <td>0.004459</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>0</td>\n",
              "      <td>0.006419</td>\n",
              "      <td>0.004713</td>\n",
              "      <td>0.002471</td>\n",
              "      <td>0.023536</td>\n",
              "      <td>0.015724</td>\n",
              "      <td>0.004699</td>\n",
              "      <td>0.015647</td>\n",
              "      <td>0.003927</td>\n",
              "      <td>0.261645</td>\n",
              "      <td>0.862110</td>\n",
              "      <td>0.006394</td>\n",
              "      <td>0.004710</td>\n",
              "      <td>0.449850</td>\n",
              "      <td>0.132998</td>\n",
              "      <td>0.430219</td>\n",
              "      <td>0.007150</td>\n",
              "      <td>0.732139</td>\n",
              "      <td>0.002233</td>\n",
              "      <td>0.061547</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>1</td>\n",
              "      <td>0.898027</td>\n",
              "      <td>0.924518</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      condition         0         1  ...        33        34        35\n",
              "68            1  0.740276  0.022239  ...  0.000000  0.000000  0.000000\n",
              "505           0  0.170613  0.014809  ...  0.000000  0.000000  0.000000\n",
              "1600          1  0.855699  0.000000  ...  0.000000  0.000000  0.000000\n",
              "942           1  0.009630  0.065301  ...  0.000000  0.000000  0.000000\n",
              "322           1  0.006917  0.002116  ...  0.000000  0.000000  0.000000\n",
              "111           1  0.000403  0.003856  ...  0.000000  0.000000  0.000000\n",
              "1101          0  0.016545  0.269982  ...  0.006962  0.007356  0.003480\n",
              "411           0  0.004221  0.093995  ...  0.000000  0.000000  0.000000\n",
              "1189          0  0.006527  0.002126  ...  0.879532  0.001244  0.003442\n",
              "1435          1  0.998414  0.157498  ...  0.000000  0.000000  0.000000\n",
              "467           0  0.978252  0.000000  ...  0.000000  0.000000  0.000000\n",
              "1086          1  0.022945  0.189791  ...  0.000000  0.000000  0.000000\n",
              "1102          1  0.456257  0.007609  ...  0.000000  0.000000  0.000000\n",
              "769           1  0.975572  0.986297  ...  0.000000  0.000000  0.000000\n",
              "1399          1  0.041337  0.007085  ...  0.011010  0.000741  0.032095\n",
              "736           0  0.012352  0.013663  ...  0.000000  0.000000  0.000000\n",
              "1049          0  0.001847  0.016920  ...  0.000000  0.000000  0.000000\n",
              "1075          1  0.040182  0.179082  ...  0.000000  0.000000  0.000000\n",
              "638           0  0.006419  0.004713  ...  0.000000  0.000000  0.000000\n",
              "923           1  0.898027  0.924518  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[20 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 641
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmAZ-dlDDpNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_numeric.to_csv('/content/drive/My Drive/depressionrnn/sent_numeric_final.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osAOZW_cQXmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDZVX32MQCpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 36 #Always Check\n",
        "output_size = 2\n",
        "hidden_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2jIVT-0P9Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
        "        #self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = F.sigmoid(self.fc2(x))\n",
        "        #x = F.sigmoid(self.fc3(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO3yiDlDFs9o",
        "colab_type": "code",
        "outputId": "c18ec372-cffc-4001-f101-67482bb0c401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model_para = torch.load('/content/drive/My Drive/depressionrnn/parapred.pt')\n",
        "model_para.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=36, out_features=10, bias=True)\n",
              "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezia4idTa_S",
        "colab_type": "text"
      },
      "source": [
        "FINAL WORKING OF THE PROJECT!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic5SIV1oH_8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_para= \"please tell me this stupid algorithm works. I am very sad and lonely\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlEnIKXZJAla",
        "colab_type": "code",
        "outputId": "94ed6384-26ad-4104-ac6a-78270f5858e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sent_tokens = sent_tokenize(user_para)\n",
        "numeric_symptoms_sent_list=[]\n",
        "length= 0\n",
        "for sentence in sent_tokens:\n",
        "  print(sentence)\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  tensor = torch.LongTensor(indexed)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  prediction = torch.sigmoid(model(tensor))\n",
        "  numeric_symptoms_sent_list.append(prediction.item())\n",
        "print(numeric_symptoms_sent_list)\n"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "please tell me this stupid algorithm works.\n",
            "I am very sad and lonely\n",
            "[0.21939747035503387, 0.8963074088096619]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "devWTdNjO3bD",
        "colab_type": "code",
        "outputId": "41565960-07ef-4329-d197-ba2ff302a4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "length = len(numeric_symptoms_sent_list)\n",
        "if length<36:\n",
        "  for i in range(length,36):\n",
        "    numeric_symptoms_sent_list.append(0.0)   \n",
        "sample = np.array(numeric_symptoms_sent_list)\n",
        "sample_tensor = torch.from_numpy(sample).float()\n",
        "out = model_para(sample_tensor)\n",
        "_,predicted = torch.max(out.data,-1)\n",
        "\n",
        "if predicted.item()==1:\n",
        "  print(\"depression symptoms present\")\n",
        "else:\n",
        "  print(\"depression symptoms absent\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "depression symptoms present\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz69VZCjRbAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}